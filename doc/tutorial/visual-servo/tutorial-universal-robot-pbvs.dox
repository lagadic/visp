/**

\page tutorial-universal-robot-pbvs Tutorial: PBVS with a robot from Universal Robots
\tableofcontents

This tutorial explains also how to implement a position-based visual-servoing with an UR robot equipped with an Intel Realsense camera.  It follows \ref tutorial-universal-robot-ibvs that explains also how to setup the robot.

\section ur_pbvs Position-based visual servoing

An example of position-based visual servoing using a robot from Universal Robots equipped with a Realsense camera is available in servoUniversalRobotsPBVS.cpp.

- Attach your Realsense camera to the robot end-effector. To this end, we provide a CAD model of a support that could be 3D printed. The FreeCAD model is available [here](https://github.com/lagadic/visp/tree/master/example/servo-universal-robots).
- Put an Apriltag in the camera field of view
- If not already done, follow \ref tutorial-calibration-extrinsic to estimate \f$^e{\bf M}_c\f$, the homogeneous transformation between robot end-effector and camera frame. We suppose here that the file is located in `tutorial/calibration/eMc.yaml`.

Now enter in `example/servo-universal-robots folder` and run `servoUniversalRobotsPBVS` binary using `--eMc` to locate the file containing the \f$^e{\bf M}_c\f$ transformation. Other options are available. Using `--help` show them:

\verbatim
$ cd example/servo-universal-robots
$ ./servoUniversalRobotsPBVS --help
./servoUniversalRobotsPBVS [--ip <default 192.168.1.1>] [--tag_size <marker size in meter; default 0.12>] \
                           [--eMc <eMc extrinsic file>] [--quad_decimate <decimation; default 2>]         \
                           [--adaptive_gain] [--plot] [--task_sequencing] [--no-convergence-threshold]    \
                           [--verbose] [--help] [-h]
\endverbatim

Run the binary activating the plot and using a constant gain:

\verbatim
$ ./servoUniversalRobotsPBVS --eMc ../../tutorial/calibration/ur_eMc.yaml --plot
\endverbatim

Use the left mouse click to enable the robot controller, and the right click to quit the binary.

At this point the behaviour that you should observe is the following:

\image html img-franka-pbvs-start.png Legend: Example of initial position. The goal is here to bring the RGB frame attached to the tag over the yellow frame corresponding to the desired position of the tag in the camera frame.

\image html img-franka-pbvs-converge.png Legend: Example of final position reached after position-based visual servoing. In green, you can see the trajectories in the image of the tag corners and tag cog. The latest correspond to the trajectory of the projection in the image of the tag frame origin. The 3D trajectory of this frame is a straight line when the camera extrinsic parameters are well calibrated.

\image html img-franka-pbvs-converge-curves.png  Legend: Corresponding visual-features (translation and orientation of the \e cdMc homogeneous matrix corresponding to the transformation between the desired camera pose and the current one) and velocities applied to the robot in the camera frame. You can observe an exponential decrease of the visual features.

You can also activate an adaptive gain that will make the convergence faster:

\verbatim
$ ./servoUniversalRobotsPBVS --eMc ../../tutorial/calibration/ur_eMc.yaml --plot --adaptive_gain
\endverbatim

You can also start the robot with a zero velocity at the beginning introducing task sequencing option:

\verbatim
$ ./servoUniversalRobotsPBVS --eMc ../../tutorial/calibration/ur_eMc.yaml --plot --task_sequencing
\endverbatim

And finally you can activate the adaptive gain and task sequencing:

\verbatim
$ ./servoUniversalRobotsPBVS --eMc ../../tutorial/calibration/ur_eMc.yaml --plot --adaptive_gain --task_sequencing
\endverbatim

\section ur_pbvs_next Next tutorial

To learn more about adaptive gain and task sequencing see \ref tutorial-boost-vs.

*/
