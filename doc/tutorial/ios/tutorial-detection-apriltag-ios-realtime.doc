/**

\page tutorial-detection-apriltag-ios-realtime Tutorial: AprilTag marker real-time detection on iOS
\tableofcontents

\section intro_apriltag_ios_realtime Introduction

This tutorial follows \ref tutorial-detection-apriltag-ios and shows how to detect AprilTag markers in real time.

In this tutorial, you will be able to learn how to detect with Swift 4 and get camera intrinsic parameters.

All the material (Xcode project) described in this tutorial is part of ViSP source code and could be downloaded using the following command:

\code
$ svn export https://github.com/lagadic/visp.git/trunk/tutorial/ios/AprilTagLiveCamera
\endcode

Once downloaded, you have just to drag & drop ViSP and OpenCV frameworks available following \ref tutorial-install-ios-package.

\section videocapture_for_realtime VideoCapture for real time detection

Real-time capture of video can be achieved with AVFundation framework. You just apply the detection process you learned from the previous tutorial to each captured image.

This is capturing code in `VideoCapture.swift`:

\snippet VideoCapture.swift captureOutput

Pass image to Image process methods in `ViewController.swift`:

\snippet ViewController.swift imageDidCapture

The cameraâ€™s intrinsic parameters can be acquired from each captured image by setting `isCameraIntrinsicMatrixDeliveryEnabled` to true in the connection settings in `VideoCapture.swift`:

\snippet VideoCapture.swift camera parameters

Note: intrinsic parameters are only supported on some iOS devices with iOS11.

The intrinsic parameters that represent camera features can generally be represented by a matrix of pixel-based focal lengths and principal points (axis centers) in the image. 
The documentation for Swift is [here](https://developer.apple.com/documentation/avfoundation/avcameracalibrationdata/2881135-intrinsicmatrix). 
Since the principal point almost coincides with the image center, this tutorial uses only the focal length.

\section call_objectivec_swift Call Objective-C class from Swift

Let us consider the Xcode project named `AprilTagLiveCamera` that is part of ViSP source code and located in `$VISP_WS/tutorial/ios/AprilTagLiveCamera`. 

To open this application, if you followed \ref tutorial-install-ios-package simply run:
\code
$ cd ~/framework
$ svn export https://github.com/lagadic/visp.git/trunk/tutorial/ios/AprilTagLiveCamera
$ open ~/visp-ws/AprilTagLiveCamera -a Xcode
\endcode

or if you already downloaded ViSP following \ref tutorial-install-iOS run:
\code
$ open ~/framework/visp/tutorial/ios/AprilTagLiveCamera -a Xcode
\endcode

As described in \ref tutorial-detection-apriltag-ios once opened, you have just to drag & drop ViSP and OpenCV frameworks available in `~/framework/ios` if you followed \ref tutorial-install-ios-package.

ViSP's AprilTag detection class is currently written in Objective-C.
In order to use this Objective-C class from Swift, setting of Bridging-Header is necessary.
Look for the `"Objective-C Bridging Header"` column in the `"Build Settings"`, and make sure that "VispDetector.h", which describes the detector class this time, is set.
\image html img-detection-apriltag-realtime-ios-call-objc.jpg

The content of `VispDetector.h` is the following:
\include AprilTagLiveCamera/VispDetector.h

and in `ViewController.swift`you have the following code:
\include AprilTagLiveCamera/ViewController.swift

\section distance_from_camera Distance from camera

Detection and drawing processing is processed in `VispDetector.mm`.
For details on the detection process, see \ref tutorial-detection-apriltag and on how to draw the pose of a tag, see the previous \ref tutorial-detection-apriltag-ios.

The distance from the iOS device to the marker can be accurately detected if the tag size is properly set. 
The distance can be obtained using the getTranslationVector() method from the homogeneous transformation matrix (`cMo_vec`) representing the pose with rotation (R) and position (t) of the marker in camera coordinates. 
See here for more information vpHomogeneousMatrix class

This is achieved in `VispDetector.mm`:
\code
for(int i=0; i < NumberOfTags; i++){
    vpTranslationVector trans = cMo_vec[i].getTranslationVector();
    float x = trans[0];
    float y = trans[1];
    float z = trans[2];
    std::cout << x << y << z << std::endl;
}
\endcode

\section apriltag_ios_realtime_output Application output

Like this image below, you can see the tag id, posture and position from the camera in real time with the video captured image.
\image html img-detection-apriltag-realtime-ios-output.jpg

*/
